==> Liveness Probes: 
----------------------
        using this we can  restart the container if it fails. this is a kind of health check. the process is running but the application is not accessed by the end user. that time
 will do the health check and here the readiness probe will stop the traffic to move to that pod. and liveness probe will restart the container/application. 

Rediness Probe will going to remove the end point from the load balancer to that service. will not get that end point unless and until we get the rediness probe successeful. if its 
added the service will not send the traffic to that pods. 

=> Types of Liveness probes: (serach in document)
 1)httpGet: if our applicatipn is executed succesfully means will get 200 code. if not will get different code. 
      EX:  httpGet:
             path: /healthz
             port: 8080

2) Command: if the command executes successfully the process of liveness probe is successful. 
     EX:      command:
                - cat
                - /tmp/healthy
3) tcpSocket: here it will check the given port is accessible or not. 
     EX:     tcpSocket:
                port: 8080     

Now lets execute javwebapp server. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080

We are executung in EKS. so take any public IP from node group. we get tomcat server. 

Now we are intentionally deleting my war file from one pod. first will check the war file location. using below command

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q ls webapps
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
ROOT
docs
examples
host-manager
java-web-app
java-web-app.war
manager

Now delete the war file java-web-app.war. 

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q rm webapps/java-web-app.war
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-697f8d7b9f-bd8hw   0/1     Pending   0          19m
javawebappdeployment-697f8d7b9f-nhp2q   1/1     Running   0          19m


after deleting also the pods are still running. if we browse will ger 404 error. it means service is pushing the request to that failed pod only. this kind of scenarios will be 
handled by Liveness and readiness probes. 

ubuntu@ip-172-31-6-100:~$ kubectl get svc
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
javawebappsvc   NodePort    10.100.46.232   <none>        80:30929/TCP   21m
kubernetes      ClusterIP   10.100.0.1      <none>        443/TCP        27m
ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                172.31.43.138:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

Now we have only one end point. but we have deleted that pod. so we are gettig failed to connect that application. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 5       #the moment pod is created it will take 5 sec
          timeoutSeconds: 1           #application time out. if it crosses 1 sec. its considered as failed. 
          periodSeconds: 15          #how frequently it performs the readiness probe
        livenessProbe:
          httpGet:
            path: /java-web-app           #this context should be correct always. 
            port: 8080
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 15
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080


above we have added liveness probe and readiness probe. lets apply the kubectl again. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-6f975459b4-5cbxs   0/1     Running   0          39s
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0          39s

above pod is running but 0/1 we see. it means the readiness probe is not successful yet. 

ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


here we cant see any Endpoints. why bcoz we cant get any end point until will get the readiness probe successful. after some time will get pod and the end point also. 

ubuntu@ip-172-31-6-100:~$ kubectl describe pod javawebappdeployment-6f975459b4-5cbxs

here we can get the details of that pod creation. initialy it got failed and later it got created. it means our Rediness probe has got executed successfully. 

ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                172.31.43.138:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

Now we can see the end point in serveice. it will redirect the request to application now. browse this will get the tomcat server. 

Intentionally will remove the war file. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-6f975459b4-5cbxs   1/1     Running   0          19m
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0          19m
ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-6f975459b4-5cbxs rm webapps/java-web-app.war
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.


Now describe the svc. if we have two end points we will get only one end point here. it means the Readiness probe has removed that end point bcoz its not healthy. 

In deployment we have mentioned the Liveness probe also. Liveness intial delay sec we have mentioned is 15 sec. if application fails it will restart the conatiner/applaication.
this thing we can see in RESTART below. its 0 bcoz it hasn't restarted yet. it will restart after 15 sec. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-697f8d7b9f-bd8hw   0/1     Pending   0          19m
javawebappdeployment-697f8d7b9f-nhp2q   1/1     Running   0          19m

execute it one more time. below we can see RESTART - 1 . here after restarting the yml file it will pull that image again and create a war file.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS        AGE
javawebappdeployment-6f975459b4-5cbxs   1/1     Running   1 (4m13s ago)   24m
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0               24m


==> Scheduling: 
----------------
       We have lot of pods and how pods are scheduling? who is deciding which node pod gets scheduled? it will decide based on the resources available in my cluster. whatever node
has enough CPU and Memory. based on this it will schedule the pod. This is no mandatory to do. it will be based on requirements. 

-> Will i abale to control the pod should schedule in particulr node? 
   yes, we can do this. using below ones
NodeSelector, Nodeaffinity,taints and Tolerations. 

EX: Add resources requests and limits in our java yml file. 

resources:
  requests:
    cpu: "100m"
    memory: "1Gi"
  limits:
    cpu: "100m"
    memory: "2Gi"

after adding and applying the kubectl apply. will get pods pending.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-5cddfb799f-cjfjk   0/1     Pending   0          96s
javawebappdeployment-5cddfb799f-cr2vg   0/1     Pending   0          95s

ubuntu@ip-172-31-6-100:~$ kubectl describe pod javawebappdeployment-5cddfb799f-cjfjk

 Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  2m45s  default-scheduler  0/2 nodes are available: 1 Too many pods, 2 Insufficient memory. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..


when we describe the pod will get the answer. 0/2 nodes are available but 2 Insufficient memory (3 nodes doesn't have sufficient memory) some times will get 1 node had taint. 
bcoz scheduler will not consider master nodes. 

lets change the yml file 

resources:
  requests:
    cpu: "100m"
    memory: "256Mi"
  limits:
    cpu: "100m"
    memory: "2Gi"

Now apply and will get running pods. bcoz the requests memory has been reduced. before it was more. so not scheduled. 

=> There are some scenarios when you want your pods to end up on specific nodes.
For Ex: 

- you want your pods to end up on a machine with the SSD attached to it.
- you want to co-locate pods on a particular machine(s) from the same availablility zone.
- you want to co-locate a Pod from one service with a pod from another service on the same node bcoz these services strongly depend on each other. EX: you may want to place
a web server on the same node as the in- memory cache store like Memchached.      


1) NodeSelector:
    This is simple pod scheduling feature that allows scheduling a pod onto a node whose labesl match the nodeSelector labels specified by the user. 

check existing labels: kubectl get nodes --show-lables

here will get IP address of nodes. 
will get some labels here. it was added by defalut. but we want our own(custoem) labels here. so 

kubectl label nodes <nodename> <labelKey>=<labelValue>

kubectl label nodes ip-172-31-1-193.ap-south-1.compute.internal name=nodeOne

Now we can see this custome node in kubectl get nodes --show-lables for that IP address. 

Then add this nodeSelector under pod spec

spec: 
  nodeSelector:
    name: nodeOne

{IMP: if we have deleted the node group and EKS cluster. but now we want it again. so will go and create an EKA and node group. then we have to connect that client server in
mobaxterm/terminal and then we have execute "aws eks update-kubeconfig --name <clusterName> --region ap-south-1". then it will create kubeconfig file and connect.  

ubuntu@ip-172-31-6-100:~$ kubectl get nodes
E1125 09:22:23.791648    1096 memcache.go:265] couldn't get current server API group list: Get "https://A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com/api?timeout=32s": dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
E1125 09:22:23.794703    1096 memcache.go:265] couldn't get current server API group list: Get "https://A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com/api?timeout=32s": dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
E1125 09:22:23.797667    1096 memcache.go:265] couldn't get current server API group list: Get "https://A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com/api?timeout=32s": dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
E1125 09:22:23.800557    1096 memcache.go:265] couldn't get current server API group list: Get "https://A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com/api?timeout=32s": dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
E1125 09:22:23.803357    1096 memcache.go:265] couldn't get current server API group list: Get "https://A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com/api?timeout=32s": dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
Unable to connect to the server: dial tcp: lookup A35AF3DE87BFE5BB6D491D5413441586.gr7.ap-south-1.eks.amazonaws.com on 127.0.0.53:53: no such host
ubuntu@ip-172-31-6-100:~$ aws eks update-kubeconfig --name EKS --region ap-south-1
Updated context arn:aws:eks:ap-south-1:485210178357:cluster/EKS in /home/ubuntu/.kube/config
ubuntu@ip-172-31-6-100:~$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-172-31-1-193.ap-south-1.compute.internal    Ready    <none>   4m29s   v1.26.10-eks-e71965b
ip-172-31-37-189.ap-south-1.compute.internal   Ready    <none>   4m27s   v1.26.10-eks-e71965b}



=>NodeSelector:   

ubuntu@ip-172-31-6-100:~$ kubectl apply -f javadeployment.yml
deployment.apps/javawebappdeployment created
service/javawebappsvc unchanged
ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-5545dcff9c-bw9bz   0/1     Pending   0          16s
javawebappdeployment-5545dcff9c-g4m98   0/1     Pending   0          16s

do you know why pods are pending? why bcoz we have mentioned wrong nodeselector name. its case sensitive. so we should mention as it is. 

ubuntu@ip-172-31-6-100:~$ kubectl describe pod javawebappdeployment-5545dcff9c-bw9bz

Node-Selectors:              name=nodeone
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2m5s  default-scheduler  0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..


Above we have described the pod. and we can see the node label is wrong. it should be "name=nodeOne" thats why we are getting "2 node(s) didn't match Pod's node affinity/selector". so correct
it and will get running pods.

NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-674568bcf6-xtmsr   0/1     Running   0          22s
javawebappdeployment-674568bcf6-xzhjd   0/1     Running   0          22s


==> Node Affiity: Node selector is the simplest Pod scheduling constarint in k8s. the affinity greatly expands the nodeselector functionally introducing the fallowing improvemnts
1) Affinity launguage is more expressive. we can use more conditions. this affinity is based on node labels only. 
2) a)Soft Rule (prefered Rules)- Here eventhough we dont have the node with the mentioned label also it will schedule the pods. 
      -preferDuringSchedulingIgnoredDuringexecution

   b) Hard Rule (Required Rules)- but here the label mentioned the node should match in our yml file. then only it will schedule the pod. 
       -requiredDuringSchedulingRequiredDuringexecution: the node should match while scheduling but not during execution
       -requiredDuringSchedulingIgnoredDuringexecution: here labels should match in scheduling and execution as well. once its scheduled it should have that label, but 
once its scheduled if we remove labels also pods will continue to run. 

    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoreDuringExecution:
            nodeSelectorterms:
            - matchExpressions: 
              - key: "name"
                operator: In
                values:
                - nodeOne

Here the name of the value should match. the  only schedulerwith will schedule the pod. 

=> SoftRule:
-------------
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoreDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: name
                operator: In
                values:
                - workone

Here we dont need to match with node key value with k8s any node. above we have diff value. but ours is name=nodeOne. but here it is name=workone. but still it will 
schedule the pod. we see running pods here. 


==> POD Affinity and POD Anti Affinity:
----------------------------------------
        Here i want POD 1 and POD 2 should run on same node. then the key and value of the pod should be same and what we have in node. then we can acheive this. this is called
"POD Affinity". 
2nd scenario is like the POD1 and POD2 should not run of same node eventhough they have the same key and the value. so that time we use "POD Anti Affinity"
These two scenarios are appicable to pods which are already running on the node and based on the labels of the pods. 

- Similarly here also we have "requiredDuringSchedulingIgnoreDuringExecution" and "preferredDuringSchedulingIgnoreDuringExecution". 

EX:  affinity: 
       podAffinity: 
         requiredDuringSchedulingIgnoreDuringExecution
         - labelSelector: 
             matchExpressions: 
             - key: app
               operator: In
               values: 
               - nginx
          topologyKey: "kubernetes.io/hostname"

Above the pods have the app: nginx they will be availbel in nodes as well then only it will schedule that all same labeled pods in one node node.


EX:  affinity: 
       podAntiAffinity: 
         requiredDuringSchedulingIgnoreDuringExecution
         - labelSelector: 
             matchExpressions: 
             - key: app
               operator: In
               values: 
               - nginx
          topologyKey: "kubernetes.io/hostname"


Above one is related to POD Anti Affinity. it means if any pods with the app: nginx they should not scheduel in same node. 

@ How pods are by defalut scheduled/ on which basis they schedule?   
=> based on the CPU and memory available/ based on the resource request. 

==> Taints: 
    -------
        Node affinity is a propetry of pods that attracts them to a set of nodes. taints are opposite , they allow a node to repel (Reject) a set of pods. somtime i dont want set of 
 pods to be run on some set of nodes so in that case we use Taints. 

@ Why my pods are not scheduling in masternode? 
=> kubectl get nodes
  here will see all nodes including master node. 
kubectl describe node ip-172-31-14-131

Taints: node-role.kubenetes.io/master:NoSchedule

if we tolerate this one we can schedule a pod in master node also. 

Syntax:  kubectl taint nodes <nodename><key>=<value>:<effect>

we can give any key, any value and any effect(out of 3). 

3 effects: 
1) NoSchedule- Doesnt schedule a pod without matching toleration. 
-The Taint in opposite to nodesekector and node affinity. why bocz if we create a Taint to one node. that node will not schedule any pods in it. unless until we tolerate this 
taint it will not schedue any pods in it. If already some pods are running in that node. then if we add NoSchedule the existing pod will run but not new ones. 

2) PreferNoSchedule- Prefers that the pod without matching toleration be not scheduled on the node. it is a softer version of NoSchedule effect. 

3) NoExecute- Evicts the pods that dont have matching tolerations. 
- If already some pods are running in that node. then if we add NoExecute it will terminate all pods.

 EX: ubuntu@ip-172-31-6-100:~$ kubectl get pods -o wide
NAME                                    READY   STATUS              RESTARTS   AGE   IP       NODE                                           NOMINATED NODE   READINESS GATES
javawebappdeployment-7476df7889-qj4vb   0/1     Pending             0          9s    <none>   <none>                                         <none>           <none>
javawebappdeployment-7476df7889-qj5c2   0/1     ContainerCreating   0          9s    <none>   ip-172-31-33-148.ap-south-1.compute.internal   <none>           <none>
ubuntu@ip-172-31-6-100:~$ kubectl taint nodes ip-172-31-33-148.ap-south-1.compute.internal node=HatesPods:NoSchedule
node/ip-172-31-33-148.ap-south-1.compute.internal tainted

ubuntu@ip-172-31-6-100:~$ kubectl describe node ip-172-31-33-148.ap-south-1.compute.internal

Taints:             node=HatesPods:NoSchedule

Above we can see the nde has been tainted. the check the pods again, we can see that node ip-172-31-33-148 again. bcoz its existing one.

ubuntu@ip-172-31-6-100:~$ kubectl get pods -o wide
NAME                                    READY   STATUS    RESTARTS      AGE    IP              NODE                                           NOMINATED NODE   READINESS GATES
javawebappdeployment-7476df7889-qj4vb   0/1     Pending   0             4m8s   <none>          <none>                                         <none>           <none>
javawebappdeployment-7476df7889-qj5c2   0/1     Running   3 (51s ago)   4m8s   172.31.45.220   ip-172-31-33-148.ap-south-1.compute.internal   <none>   


Now will delete that deployment and check 

ubuntu@ip-172-31-6-100:~$ kubectl get pods -o wide
NAME                                    READY   STATUS             RESTARTS        AGE   IP              NODE                                           NOMINATED NODE   READINESS GATES
javawebappdeployment-7476df7889-qj4vb   0/1     Pending            0               16m   <none>          <none>                                         <none>           <none>
javawebappdeployment-7476df7889-qj5c2   0/1     CrashLoopBackOff   7 (3m52s ago)   16m   172.31.45.220   ip-172-31-33-148.ap-south-1.compute.internal   <none>           <none>

ubuntu@ip-172-31-6-100:~$ kubectl get deployment javawebappdeployment
NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
javawebappdeployment   0/2     2            0           17m

ubuntu@ip-172-31-6-100:~$ kubectl apply -f javadeployment.yml
deployment.apps/javawebappdeployment unchanged
service/javawebappsvc unchanged

ubuntu@ip-172-31-6-100:~$ kubectl get pods -o wide
NAME                                    READY   STATUS    RESTARTS        AGE   IP              NODE                                           NOMINATED NODE   READINESS GATES
javawebappdeployment-7476df7889-qj4vb   0/1     Pending   0               17m   <none>          <none>                                         <none>           <none>
javawebappdeployment-7476df7889-qj5c2   0/1     Running   8 (5m34s ago)   17m   172.31.45.220   ip-172-31-33-149.ap-south-1.compute.internal   <none>           <none>


Here will get different nodes now. 

==> Toleration:
      using this we can schedule a pod in taint node as well. we have to write in manifest file. 

ubuntu@ip-172-31-6-100:~$ vi javadeployment.yml 

spec: 
  tolerations: 
  - key: "node"
    operator: "Equal"
    value: "HatesPod"
    effect: "NoSchedule"


Here we can observe that we have tainted with node:HatesPod:NoSchedule. but now we are tolerating that taint in yml file. so now the pod will be scheduled in this node also. 
  
apply kubectl to yml file and then check the pods "kubectl get pods -o wide" there we can see the tainted node IP. 

==> I want remove the taint. similar comand but just add - at the end 

ubuntu@ip-172-31-6-100:~$"kubectl taint nodes ip-172-31-33-148.ap-south-1.compute.internal node=HatesPods:NoSchedule-"
node ip-172-31-33-148.ap-south-1.compute.interna untainted

@ is it possible to make one node unschedulable? yes 

=> Drain- if we drain some node, whatever pods which is already running in that node, those pods will be moved to another node. 

==> Will i able to do OS Patches, software version upgrades without impacting? yes , we can but as per the best practice dont directly Drain. first make "cordon"

=> cordon: It will make node as unschedulable. the exsting pod will stay there. no new pod will unschedule. once we done corndon. then we have to do Drain. then pods will be moved 
to other nodes. 

Uncordon: once we done the uncordon again it will schedulable. 

1) kubectl cordon ip-172-31-34-139  - here we can see ip-172-31-34-139 Ready but SchedulingDisabled. 
2) kubectl drain ip-172-31-34-139   - here it will move all existing pods to other nodes. 
3) kubectl uncordon ip-172-31-34-139 - here it will remove that SchedulingDisabled for that node. now its ready to schedule to any pods. 


Disadvantages of Selfmanaged k8s cluster: 
1) if my master m/c goes down or api server face down time
2) we dont get hih availability
3) if any worker goes down we will not get another worker automatically. 
4) we will not get another node automatically when the CPU and Memory goes down. 


==> LoadBalancer: 
   ---------------
       its one more k8s service type. in the Real time all our k8s servers will run in private server. end user cannot access using internet. we have to use "External loadbalancer"

this External LoadBalancer is outside of our k8s. its not a part of the cluster. this load balancer will route the traffic to severs using NodeIP:NodePort internally. 









