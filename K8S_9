==> Liveness Probes: 
----------------------
        using this we can  restart the container if it fails. this is a kind of health check. the process is running but the application is not accessed by the end user. that time
 will do the health check and here the readiness probe will stop the traffic to move to that pod. and liveness probe will restart the container/application. 

Rediness Probe will going to remove the end point from the load balancer to that service. will not get that end point unless and until we get the rediness probe successeful. if its 
added the service will not send the traffic to that pods. 

=> Types of Liveness probes: (serach in document)
 1)httpGet: if our applicatipn is executed succesfully means will get 200 code. if not will get different code. 
      EX:  httpGet:
             path: /healthz
             port: 8080

2) Command: if the command executes successfully the process of liveness probe is successful. 
     EX:      command:
                - cat
                - /tmp/healthy
3) tcpSocket: here it will check the given port is accessible or not. 
     EX:     tcpSocket:
                port: 8080     

Now lets execute javwebapp server. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080

We are executung in EKS. so take any public IP from node group. we get tomcat server. 

Now we are intentionally deleting my war file from one pod. first will check the war file location. using below command

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q ls webapps
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
ROOT
docs
examples
host-manager
java-web-app
java-web-app.war
manager

Now delete the war file java-web-app.war. 

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q rm webapps/java-web-app.war
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-697f8d7b9f-bd8hw   0/1     Pending   0          19m
javawebappdeployment-697f8d7b9f-nhp2q   1/1     Running   0          19m


after deleting also the pods are still running. if we browse will ger 404 error. it means service is pushing the request to that failed pod only. this kind of scenarios will be 
handled by Liveness and readiness probes. 

ubuntu@ip-172-31-6-100:~$ kubectl get svc
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
javawebappsvc   NodePort    10.100.46.232   <none>        80:30929/TCP   21m
kubernetes      ClusterIP   10.100.0.1      <none>        443/TCP        27m
ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                172.31.43.138:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

Now we have only one end point. but we have deleted that pod. so we are gettig failed to connect that application. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 5       #the moment pod is created it will take 5 sec
          timeoutSeconds: 1           #application time out. if it crosses 1 sec. its considered as failed. 
          periodSeconds: 15          #how frequently it performs the readiness probe
        livenessProbe:
          httpGet:
            path: /java-web-app           #this context should be correct always. 
            port: 8080
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 15
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080


above we have added liveness probe and readiness probe. lets apply the kubectl again. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-6f975459b4-5cbxs   0/1     Running   0          39s
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0          39s

above pod is running but 0/1 we see. it means the readiness probe is not successful yet. 

ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>


here we cant see any Endpoints. why bcoz we cant get any end point until will get the readiness probe successful. after some time will get pod and the end point also. 

ubuntu@ip-172-31-6-100:~$ kubectl describe pod javawebappdeployment-6f975459b4-5cbxs

here we can get the details of that pod creation. initialy it got failed and later it got created. it means our Rediness probe has got executed successfully. 

ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                172.31.43.138:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

Now we can see the end point in serveice. it will redirect the request to application now. browse this will get the tomcat server. 

Intentionally will remove the war file. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-6f975459b4-5cbxs   1/1     Running   0          19m
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0          19m
ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-6f975459b4-5cbxs rm webapps/java-web-app.war
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.


Now describe the svc. if we have two end points we will get only one end point here. it means the Readiness probe has removed that end point bcoz its not healthy. 

In deployment we have mentioned the Liveness probe also. Liveness intial delay sec we have mentioned is 15 sec. if application fails it will restart the conatiner/applaication.
this thing we can see in RESTART below. its 0 bcoz it hasn't restarted yet. it will restart after 15 sec. 

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-697f8d7b9f-bd8hw   0/1     Pending   0          19m
javawebappdeployment-697f8d7b9f-nhp2q   1/1     Running   0          19m

execute it one more time. below we can see RESTART - 1 . here after restarting the yml file it will pull that image again and create a war file.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS        AGE
javawebappdeployment-6f975459b4-5cbxs   1/1     Running   1 (4m13s ago)   24m
javawebappdeployment-6f975459b4-qr4hm   0/1     Pending   0               24m


==> Scheduling: 
----------------
       We have lot of pods and how pods are scheduling? who is deciding which node pod gets scheduled? it will decide based on the resources available in my cluster. whatever node
has enough CPU and Memory. based on this it will schedule the pod. 

-> Will i abale to control the pod should schedule in particulr node? 
   yes, we can do this. using below ones
NodeSelector, Nodeaffinity,taints and Tolerations. 

EX: Add resources requests and limits in our java yml file. 

resources:
  requests:
    cpu: "100m"
    memory: "1Gi"
  limits:
    cpu: "100m"
    memory: "2Gi"

after adding and applying the kubectl apply. will get pods pending.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-5cddfb799f-cjfjk   0/1     Pending   0          96s
javawebappdeployment-5cddfb799f-cr2vg   0/1     Pending   0          95s

ubuntu@ip-172-31-6-100:~$ kubectl describe pod javawebappdeployment-5cddfb799f-cjfjk

 Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  2m45s  default-scheduler  0/2 nodes are available: 1 Too many pods, 2 Insufficient memory. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..


when we describe the pod will get the answer. 0/2 nodes are available but 2 Insufficient memory (3 nodes doesn't have sufficient memory) some times will get 1 node had taint. 
bcoz scheduler will not consider master nodes. 

lets change the yml file 

resources:
  requests:
    cpu: "100m"
    memory: "256Mi"
  limits:
    cpu: "100m"
    memory: "2Gi"

Now apply and will get running pods. bcoz the requests memory has been reduced. before it was more. so not scheduled. 

=> There are some scenarios when you want your pods to end up on specific nodes.
For Ex: 

- you want your pods to end up on a machine with the SSD attached to it.
- you want to co-locate pods on a particular machine(s) from the same availablility zone.
- you want to co-locate a Pod from one service with a pod from another service on the same node bcoz these services strongly depend on each other. EX: you may want to place
a web server on the same node as the in- memory cache store like Memchached.      


1) NodeSelector:
    This is simple pod scheduling feature that allows scheduling a pod onto a node whose labesl match the nodeSelector labels specified by the user. 

check existing labels: kubectl get nodes --show-lables

will get some labels here. it was added by defalut. but we want our own(custoem) labels here. so 

kubectl label nodes <nodename> <labelKey>=<labelValue>

kubectl label nodes ip-172-31-14-131 name=nodeOne

Now we can see this custome node in kubectl get nodes --show-lables for that IP address. 















