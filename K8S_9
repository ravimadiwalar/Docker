==> Liveness Probes: 
----------------------
        using this we can  restart the container if it fails. this is a kind of health check. the process is running but the application is not accessed by the end user. that time
 will do the health check and here the readiness probe will stop the traffic to move to that pod. and liveness probe will restart the container/application. 

Rediness Probe will going to remove the end point from the load balancer to that service. will not get that end point unless and until we get the rediness probe successeful. if its 
added the service will not send the traffic to that pods. 

=> Types of Liveness probes: (serach in document)
 1)httpGet: if our applicatipn is executed succesfully means will get 200 code. if not will get different code. 
      EX:  httpGet:
             path: /healthz
             port: 8080

2) Command: if the command executes successfully the process of liveness probe is successful. 
     EX:      command:
                - cat
                - /tmp/healthy
3) tcpSocket: here it will check the given port is accessible or not. 
     EX:     tcpSocket:
                port: 8080     

Now lets execute javwebapp server. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080

We are executung in EKS. so take any public IP from node group. we get tomcat server. 

Now we are intentionally deleting my war file from one pod. first will check the war file location. using below command

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q ls webapps
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
ROOT
docs
examples
host-manager
java-web-app
java-web-app.war
manager

Now delete the war file java-web-app.war. 

ubuntu@ip-172-31-6-100:~$ kubectl exec javawebappdeployment-697f8d7b9f-nhp2q rm webapps/java-web-app.war
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.

ubuntu@ip-172-31-6-100:~$ kubectl get pods
NAME                                    READY   STATUS    RESTARTS   AGE
javawebappdeployment-697f8d7b9f-bd8hw   0/1     Pending   0          19m
javawebappdeployment-697f8d7b9f-nhp2q   1/1     Running   0          19m


after deleting also the pods are still running. if we browse will ger 404 error. it means service is pushing the request to that failed pod only. this kind of scenarios will be 
handled by Liveness and readiness probes. 

ubuntu@ip-172-31-6-100:~$ kubectl get svc
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
javawebappsvc   NodePort    10.100.46.232   <none>        80:30929/TCP   21m
kubernetes      ClusterIP   10.100.0.1      <none>        443/TCP        27m
ubuntu@ip-172-31-6-100:~$ kubectl describe svc javawebappsvc
Name:                     javawebappsvc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=javawebapp
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.46.232
IPs:                      10.100.46.232
Port:                     <unset>  80/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30929/TCP
Endpoints:                172.31.43.138:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

Now we have only one end point. but we have deleted that pod. so we are gettig failed to connect that application. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: javawebappdeployment
spec:
  replicas: 2     #internaly it will create RS
  selector:
     matchLabels:
       app: javawebapp
  strategy:
    type: Recreate   #This strategy will applicable while updating the version
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: dockerhandson/java-web-app:1
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 15
        livenessProbe:
          httpGet:
            path: /java-web-app           #this context should be correct always. 
            port: 8080
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 15
---
apiVersion: v1
kind: Service
metadata:
  name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080


above we have added liveness probe and readiness probe. 
