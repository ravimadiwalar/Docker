Name Space: suppose if we have two projcets. i can host/deploy project 1 in one name space and other diff namespace. logically they are separated by namespace. 
Name space is like a Cluster inside the Cluster for isolation of k8s resources. 

* To delete the pod: kubectl delete pod <podname>

Two containers in single pod will communicate with other by localHost. but one pod connect with other pod inside the cluster (they may in single mc or different mc)
by Service resource. we will not use Pod IP. pod may go down. so we use service name. 

One microservice pod communicate with other microservice pod withing the cluster(they may in single mc or different mc). we use ClusterIP service type. 
         <ServiecName>:<ServicePort>

From outside we use Node IP. <NodeIP>:<NodePort>

Pod Lifecycle:
*make a Pod to API server using a local pod definition file(Manifest file).
*The API Server saves the info for the pod in ETCD.
*The scheduler finds the unscheduled pods and schedules it to node.
*Kubelet running on the node. sees the pod scheduled and fires up docker (To pull the images and create the container).
*docker runs the container.
*The entire lifecycle state of the pod is stored in ETCD.

Pod Concepts:
*Pod is ephemeral(lasting for a very short time) and wont be rescheduled to a new node onec dies. (Pod acn go down for any reason/any time)
*You should not directly create/use pod for deployment, k8s have controllers like Replication controller,Replica Set,Deployment and Deamon sets to keep pod alive.


Pod Model Types:
Most often , when we deploy a pod to a k8s cluster, it will contain  a single container.but there are instaces when you might need to deploy a pod withe mupltiple containers. 

1)One-container-per-pod: This kodel is the most popular. POD is the Wrapper for a single container. 
1)Multi-container-pod or Sidecar Containers: in this model, a pod holds multiple co-located containers primary  container and utility container that helps or enhance how an application
functions (example of sidecar containers are log shippers/watchers and monitoring agents). 

Scenaria: lets assume i have a container (primary application). i want to send some logs to extrnal storage instaed of updatting the code in that container. In sidecar containers 
we got one more container is "Helper Container". it will take that task and execute it. then it will store in Volume. Volume is connected with both containers. so our primary 
container will take consider that executed task and just process it. 

In Side car Container it should have one helper container and it will help to enhance the functionality of the primary container. this is called "Co-located". and in single pod 
both container should have same storage and network. which means all containers can communicate with each other on localHost in same POD. 

3) InitContainers: 
  Before starting my main container i want execute some functionality in that pod. it will not continue to run. it will just execute some logic and onec its done our main container
will start. its one time acativity to load some files before starting the main conatiners in the pod. This will be used in both type of containers. 

4) Static Pods: 
                Static pods are directly managed by the Kubelet. and the API server doesn't have any control over these pods. The kubelet is responsible to watach each static Pod
and restart itit if it crashes. The static pods running on a node are visible on the API server. static pods doesn't have any associated replication controller. kubelet service 
itself watches it and restart it when it crahses. There is no health check for static pod. 


ubuntu@ip-172-31-33-243:~$ kubectl get all -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
pod/coredns-74ff55c5b-9jmdj                    1/1     Running   6          12d
pod/coredns-74ff55c5b-j9ktx                    1/1     Running   6          12d
pod/etcd-ip-172-31-33-243                      0/1     Running   6          12d
pod/kube-apiserver-ip-172-31-33-243            1/1     Running   6          12d
pod/kube-controller-manager-ip-172-31-33-243   0/1     Running   6          12d
pod/kube-proxy-gnw5j                           1/1     Running   0          46h
pod/kube-proxy-zfxrk                           1/1     Running   6          12d
pod/kube-scheduler-ip-172-31-33-243            0/1     Running   6          12d
pod/weave-net-bkldg                            2/2     Running   13         12d
pod/weave-net-flzl2                            2/2     Running   0          46h

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   12d

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   2         2         1       2            1           kubernetes.io/os=linux   12d
daemonset.apps/weave-net    2         2         1       2            1           <none>                   12d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           12d

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-74ff55c5b   2         2         2       12d


Above we can see kube-proxy and weave-net. they will be managed by daemonset.apps. its not managed by kubelet. if some pods go down . these Daemonset will recrete and manage the pods. 
and there are pod/coredns-74ff55c5b-9jmdj and pod/coredns-74ff55c5b-j9ktx. these Coredns will be managed by deployment.apps (deployment.apps/coredns). 

Now we can se these will be managed by control Plane. for kube-proxy, weave-net and coredn we can see controllers. but for ETCD,kube-apiserver, kube-controller-manager and kube-scheduler
we cant see any controllers. these will be managed by kubelet process directly. 

The main Use for static pod is to Run Self-hosted control plane. we can also create our own static pods. in verey node (master/worker) we can see one folder called 
"sudo ls /etc/kubernetes/manifests/" if we keep our yml file inside this manifest folder. it will create a static pod. if any pod goes down it will create its own. kubelet will
manage that. 

Why cant we create own application pod as a static pod? 
-> We cant scale my pod, cant update our pods. 

==>Replication Controller:
----------------------------
RC is one of the key feature of K8s, which is responsible for managing the pods lifecycle. it is responsible for making sure that the specified number of pod replicas are running 
at any point of time.

A RC is a structure that enables you to create multiple pods. the make sure that number of pods always exists. if pod does crash, the RC will replace it. 

RC and pods are associated with labels. 

Creating a RC with count of 1 ensures that a pod is always available. 

Rc will be created using Manifest file (yml file). 

*To know the apiVersions of all use command: kubectl api-resources

Syntax:

apiSever: v1
kind: ReplicationController
metadata: 
  name: <RCName>
  namespace: <namespaceName>     --> if we want to specify the RC in diff namespace. 
  labels: 
    <key>: <value>        ---> its optinal. 
spec:
  replicas: <numberofReplicas>       ---> If i dont mention this line. it will craete one replica by default. if we mention more. it will create that much. 
  selectors:  
    <key>: <value>  #POD labels      ---> RC will manage and idetify the pods based on labels and selectors. 
  template: # POD template
    metadata:                         --> This meatadata is for our pod.
      name: <Podname>
      labels:
         <key>: <value>
   spec: 
     containers: 
     - name: <containerName>
       image: <imageName>
       ports:
       - containerPort: <PortName>        ---> Repeat spec if we want muptiple conatiners from spec    


Lets create a manifest file: 

ubuntu@ip-172-31-33-243:~$ vi javawebapprc.yml 
apiVersion: v1
kind: ReplicationController
metadata: 
  name: javawebapprc
  namespace: flipkartns   --> i just mentioned one random name now. but in rea time its should be related.
spec:
  replicas: 2     --> 2 pods will create.
  selectors: 
    app: javawebapp
template:                 --> under template we are filling pod details. RC will be used to create a pod only. thats why we dont mention apiversionand kind under template again. 
  metadata: 
    name: javawebapppod   --> eventhough i m giving name our pod will not create in this name.
    labels: 
      app: javawebapp
  spec: 
    containers: 
    - name: javawebappcontainer
      image: dockerhandson/java-web-app:1    --->suppose instead of 1 you give non existing image version. we will get pull backup error.
      ports: 
      - containerPort: 8080




  














