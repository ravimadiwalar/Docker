Name Space: suppose if we have two projcets. i can host/deploy project 1 in one name space and other diff namespace. logically they are separated by namespace. 
Name space is like a Cluster inside the Cluster for isolation of k8s resources. 

* To delete the pod: kubectl delete pod <podname>

Two containers in single pod will communicate with other by localHost. but one pod connect with other pod inside the cluster (they may in single mc or different mc)
by Service resource. we will not use Pod IP. pod may go down. so we use service name. 

One microservice pod communicate with other microservice pod withing the cluster(they may in single mc or different mc). we use ClusterIP service type. 
         <ServiecName>:<ServicePort>

From outside we use Node IP. <NodeIP>:<NodePort>

Pod Lifecycle:
*make a Pod to API server using a local pod definition file(Manifest file).
*The API Server saves the info for the pod in ETCD.
*The scheduler finds the unscheduled pods and schedules it to node.
*Kubelet running on the node. sees the pod scheduled and fires up docker (To pull the images and create the container).
*docker runs the container.
*The entire lifecycle state of the pod is stored in ETCD.

Pod Concepts:
*Pod is ephemeral(lasting for a very short time) and wont be rescheduled to a new node onec dies. (Pod acn go down for any reason/any time)
*You should not directly create/use pod for deployment, k8s have controllers like Replication controller,Replica Set,Deployment and Deamon sets to keep pod alive.


Pod Model Types:
Most often , when we deploy a pod to a k8s cluster, it will contain  a single container.but there are instaces when you might need to deploy a pod withe mupltiple containers. 

1)One-container-per-pod: This Model is the most popular one. POD is the Wrapper for a single container. 
1)Multi-container-pod or Sidecar Containers: in this model, a pod holds multiple co-located containers primary  container and utility container that helps or enhance how an application
functions (example of sidecar containers are log shippers/watchers and monitoring agents). 

Scenaria: lets assume i have a container (primary application). i want to send some logs to extrnal storage instaed of updatting the code in that container. In sidecar containers 
we got one more container is "Helper Container". it will take that task and execute it. then it will store in Volume. Volume is connected with both containers. so our primary 
container will take consider that executed task and just process it. 

In Side car Container it should have one helper container and it will help to enhance the functionality of the primary container. this is called "Co-located". and in single pod 
both container should have same storage and network. which means all containers can communicate with each other on localHost in same POD. 

3) InitContainers: 
  Before starting my main container i want execute some functionality in that pod. it will not continue to run. it will just execute some logic and onec its done our main container
will start. its one time acativity to load some files before starting the main conatiners in the pod. This will be used in both type of containers. 

4) Static Pods: 
                Static pods are directly managed by the Kubelet. and the API server doesn't have any control over these pods. The kubelet is responsible to watach each static Pod
and restart it if it crashes. The static pods running on a node are visible on the API server. static pods doesn't have any associated replication controller. kubelet service 
itself watches it and restart it when it crahses. There is no health check for static pod. 


ubuntu@ip-172-31-33-243:~$ kubectl get all -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
pod/coredns-74ff55c5b-9jmdj                    1/1     Running   6          12d
pod/coredns-74ff55c5b-j9ktx                    1/1     Running   6          12d
pod/etcd-ip-172-31-33-243                      0/1     Running   6          12d
pod/kube-apiserver-ip-172-31-33-243            1/1     Running   6          12d
pod/kube-controller-manager-ip-172-31-33-243   0/1     Running   6          12d
pod/kube-proxy-gnw5j                           1/1     Running   0          46h
pod/kube-proxy-zfxrk                           1/1     Running   6          12d
pod/kube-scheduler-ip-172-31-33-243            0/1     Running   6          12d
pod/weave-net-bkldg                            2/2     Running   13         12d
pod/weave-net-flzl2                            2/2     Running   0          46h

NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   12d

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   2         2         1       2            1           kubernetes.io/os=linux   12d
daemonset.apps/weave-net    2         2         1       2            1           <none>                   12d

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           12d

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-74ff55c5b   2         2         2       12d


Above we can see kube-proxy and weave-net. they will be managed by daemonset.apps. its not managed by kubelet. if some pods go down . these Daemonset will recrete and manage the pods. 
and there are pod/coredns-74ff55c5b-9jmdj and pod/coredns-74ff55c5b-j9ktx. these Coredns will be managed by deployment.apps (deployment.apps/coredns). 

Now we can see these will be managed by control Plane. for kube-proxy, weave-net and coredn we can see controllers. but for ETCD,kube-apiserver, kube-controller-manager and kube-scheduler
we cant see any controllers. these will be managed by kubelet process directly. 

The main Use for static pod is to Run Self-hosted control plane. we can also create our own static pods. in every node (master/worker) we can see one folder called 
"sudo ls /etc/kubernetes/manifests/" if we keep our yml file inside this manifest folder. it will create a static pod. if any pod goes down it will create its own. kubelet will
manage that. 

Why cant we create own application pod as a static pod? 
-> We cant scale my pod, cant update our pods. 

==>Replication Controller:
----------------------------
RC is one of the key feature of K8s, which is responsible for managing the pods lifecycle. it is responsible for making sure that the specified number of pod replicas are running 
at any point of time.

A RC is a structure that enables you to create multiple pods. the make sure that number of pods always exists. if pod does crash, the RC will replace it. 

RC and pods are associated with labels. 

Creating a RC with count of 1 ensures that a pod is always available. 

Rc will be created using Manifest file (yml file). 

*To know the apiVersions of all use command: kubectl api-resources

Syntax:

apiSever: v1
kind: ReplicationController
metadata: 
  name: <RCName>
  namespace: <namespaceName>     --> if we want to specify the RC in diff namespace. 
  labels: 
    <key>: <value>        ---> its optinal. 
spec:
  replicas: <numberofReplicas>       ---> If i dont mention this line. it will craete one replica by default. if we mention more. it will create that much. 
  selectors:  
    <key>: <value>  #POD labels      ---> RC will manage and idetify the pods based on labels and selectors. 
  template: # POD template
    metadata:                         --> This meatadata is for our pod.
      name: <Podname>
      labels:
         <key>: <value>
   spec: 
     containers: 
     - name: <containerName>
       image: <imageName>
       ports:
       - containerPort: <PortName>        ---> Repeat spec if we want muptiple conatiners from spec    


Lets create a manifest file: 

ubuntu@ip-172-31-33-243:~$ vi javawebapprc.yml 
apiVersion: v1
kind: ReplicationController
metadata: 
  name: javawebapprc
  namespace: flipkartns   --> i just mentioned one random name now. but in rea time its should be related.
spec:
  replicas: 2     --> 2 pods will create.
  selectors: 
    app: javawebapp
template:                 --> under template we are filling pod details. RC will be used to create a pod only. thats why we dont mention apiversion and kind under template again. 
  metadata: 
    name: javawebapppod   --> eventhough i m giving name our pod will not create in this name.
    labels: 
      app: javawebapp
  spec: 
    containers: 
    - name: javawebappcontainer
      image: dockerhandson/java-web-app:1    --->suppose instead of 1 you give non existing image version. we will get pull backup error.
      ports: 
      - containerPort: 8080


Now we have created in flipkartns name space and pods will be created in that name only. and create a service in flipkartns name space. 

apiVersion: v1
kind: ReplicationController
metadata: 
  name: javawebapprc
  namespace: flipkartns   
spec:
  replicas: 2     
  selector: 
    app: javawebapp
template:                  
  metadata: 
    name: javawebapppod   
    labels: 
      app: javawebapp
  spec: 
    containers: 
    - name: javawebappcontainer
      image: dockerhandson/java-web-app:1    
      ports:          
      - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata: 
  name: javawebappsvc
  namespace: flipkartns  --> if we dont mention also it will craete in the same name space.
spec: 
  type: NodePort
  selector: 
    app: Javawebapp
  ports:
  - port: 80
    targetPort: 8080   --> targetPort depends on your container port. 

:wq

before applying this check do we have any namespaces already. 

ubuntu@ip-172-31-33-243:~$ kubectl get ns   --> here will get default namspaces
NAME              STATUS   AGE
default           Active   16d
kube-node-lease   Active   16d
kube-public       Active   16d
kube-system       Active   16d

we can create our own namespaces also 

ubuntu@ip-172-31-33-243:~$ kubectl create ns flikartns
namespace/flipkartns created
ubuntu@ip-172-31-33-243:~$kubectl get ns  --> now we can see flipkartns
NAME              STATUS   AGE
default           Active   16d
flipkartns        Active   4s
kube-node-lease   Active   16d
kube-public       Active   16d
kube-system       Active   16d

Under manifest file we have alaredy mantioned the namespace.

To check any resources in flipkart namespace: 
ubuntu@ip-172-31-33-243:~$kubectl get all -n flipkartns
No resource found in flipkartns namespace

Now apply this : ubuntu@ip-172-31-33-243:~$kubectl apply -f javawebapprc.yml 
replicationcontroller/javawebapprc created
service/javawebapprc created 

* in the yml file itself we can create a namespace (on the top side).  

apiVersion: v1
kind: Namespace
metadata:
  name: flipkartns
---

ubuntu@ip-172-31-33-243:~$kubectl get rc
No resource found

we cant see our replication controller here. why bcoz this is pointing to default namespace. so 

ubuntu@ip-172-31-33-243:~$ kubectl get rc -n flipkartns
NAME           DESIRED   CURRENT   READY   AGE
javawebapprc   2         2         0       71s

now we cann see our replication controller with 2 pods. if we observe the pod names. both are unique. here we cann service port as well which was assigned by k8s.

To know the Nodeport:
ubuntu@ip-172-31-33-243:~$ kubectl get svc -n flipkartns
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
javawebappsvc   NodePort   10.98.218.30   <none>        80:31140/TCP   7m9s

lets go to aws and open that port first. and take that public IP: 13.233.343.8:31140/jav-web-app and browse this from outside. 

To check pods under flipkartns: kubectl get pods -n flipkartns 
ubuntu@ip-172-31-33-243:~$ kubectl get pods -n flipkartns
NAME                 READY   STATUS    RESTARTS   AGE
javawebapprc-p2hjd   0/1     Pending   0          4m40s
javawebapprc-r5bsp   0/1     Pending   0          4m40s

*Delete one pod and browse outside "13.233.343.8:30555/jav-web-app". we can see its opened. but we have deleted. how? RC has auomatically created one more pod. 

ubuntu@ip-172-31-33-243:~$ kubectl get pods -n flipkartns 


now we can see new pod which was created recently(check the age). 

#i dont want to give contect at the end of every command "-n flipkartns". so we need to set flipkart as a fixed namespace. to do that

ubuntu@ip-172-31-33-243:~$ kubectl config set-context --current --namespace=flipkartns
Context "kubernetes-admin@kubernetes" modified.

now we have set the fixed namspace. whatever you do now it will execute under flipkartns namespace. 

to chek that: ubuntu@ip-172-31-33-243:~$ kebectl config get-context 

we can change also : ubuntu@ip-172-31-33-243:~$ kubectl config set-context --current --namespace=default 

Just change the context to Flipkartns again and see the pods. we have two pods. 

ubuntu@ip-172-31-33-243:~$ kubectl config set-context --current --namespace=flipkartns
Context "kubernetes-admin@kubernetes" modified.
ubuntu@ip-172-31-33-243:~$ kubectl get all
NAME                     READY   STATUS    RESTARTS   AGE
pod/javawebapprc-p2hjd   0/1     Pending   0          23h
pod/javawebapprc-r5bsp   0/1     Pending   0          23h

NAME                                 DESIRED   CURRENT   READY   AGE
replicationcontroller/javawebapprc   2         2         0       23h

NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/javawebappsvc   NodePort   10.98.218.30   <none>        80:31140/TCP   23h


Here we can scale the pods count. ==> kubectl scale rc <rcname> --replicas=<count>

ubuntu@ip-172-31-33-243:~$ kubectl scale rc javawebapprc --replicas=3
replicationcontroller/javawebapprc scaled
ubuntu@ip-172-31-33-243:~$ kubectl get all
NAME                     READY   STATUS    RESTARTS   AGE
pod/javawebapprc-dvcbk   0/1     Pending   0          4s             --> why am i getting 0/1 instead of 1/1?
pod/javawebapprc-p2hjd   0/1     Pending   0          23h
pod/javawebapprc-r5bsp   0/1     Pending   0          23h

NAME                                 DESIRED   CURRENT   READY   AGE
replicationcontroller/javawebapprc   3         3         0       23h

NAME                    TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/javawebappsvc   NodePort   10.98.218.30   <none>        80:31140/TCP   23h

now we can see 3 pods and desired and current its showing 3. 















