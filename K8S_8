==> Type of Autoscaling:
1) Vartical scaling: The capacity of the server will increase. for eg. if we have 4 core 8GB. we can change that to 8 core 32 GB. 
2) Horizontal Scaling: here we will increse the servers with same capacity. will attached these with load balanacer and it will route all the traffic equally. 

In K8s we can do both Vertical and Horizontal Scaling. 

=> POD Autoscaler:
1) HPA: Horizontal POD Autoscaler- It automatically scales the No of pods i a RC, Deployment,RS based on observed CPU/memory utilization. This HPA is implemented as a K8S API resource
and controller. The resource determines the behaviour of the controller. the controller periodically adjust the no of replicas in a RC or deployment to match the observed avaerage
CPU/memory utilization to the target specified by the used. 

HPA will interact with Metric Server to identify cpu/memeory utilization of POD. 

2) VPA: Vericatl POD Autoscaler

@ can i set limits to our pods (CPU and Memory)?
-> Yes

@ If process(application) is running out of CPU - Performance will be impacted/ process will hung. will not get quick response. 

@ If process is running out of memory- process will be killed

@ Can we achive POD Autoscaler in self managed k8s cluster?
-> YES

==> Kubectl top nodes OR kubectl top pods - we will get an error like "Metrics API not available"
Metrics means the CPU and Memory utilization. we have to know this metrics then only we can do autoscaling. We dont have this in K8S by defalut. we have to donload Metrics application.

ubuntu@ip-172-31-34-33:~$ kubectl top pods
error: Metrics API not available
ubuntu@ip-172-31-34-33:~$ kubectl top nodes
error: Metrics API not available

@ What is heapster?
-> In previous version of K8s we have heapster as a defalut one to find out CPU/memory utilization. 

@ What is metrics Server?
-> Instead of Heapster we have Metrics Server. 


==> K8s Metric Server: 
           Its an application that collects metric from objects such as PODs, nodes accordib to the state of CPU, RAM and keeps them in time. This will be installed in K8S as an addon. 

Metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through Metrics API for use by Horizontal Pod Autoscaler and Vertical Pod Autoscaler.

Go to browser and type "kubernetes-sigs/metrics-server"- go inside and we can see repository - Installation - copy that yml link and browse in new tab - will get yml file. then
applu kubectl apply -f 

OR
MithunTechnologiesDevOps - metrics-server  - do the fallowing. here we have 1.8 is the latest version. so apply that. In deploy file we can see all versions. 

git clone https://github.com/MithunTechnologiesDevOps/metrics-server.git 

cd metric-server

kubectl apply -f deploy/1.8+/   



Now - kubectl get all -n kube-system  - we can see metruc server running as a pod now and we can see deployment/metric server it will manage the pod. 

ubuntu@ip-172-31-34-33:~/metrics-server/deploy$ kubectl get all -n kube-system
NAME                                          READY   STATUS    RESTARTS   AGE
pod/coredns-74ff55c5b-ltltx                   1/1     Running   22         20d
pod/coredns-74ff55c5b-v44vh                   1/1     Running   22         20d
pod/etcd-ip-172-31-34-33                      1/1     Running   22         20d
pod/kube-apiserver-ip-172-31-34-33            1/1     Running   22         20d
pod/kube-controller-manager-ip-172-31-34-33   1/1     Running   22         20d
pod/kube-proxy-7dh67                          1/1     Running   16         19d
pod/kube-proxy-8mqqz                          1/1     Running   22         20d
pod/kube-proxy-d6tvl                          1/1     Running   1          20d
pod/kube-proxy-pgq84                          1/1     Running   11         15d
pod/kube-scheduler-ip-172-31-34-33            1/1     Running   22         20d
pod/metrics-server-766c9b8df-v4d4j            1/1     Running   0          61s
pod/weave-net-cf2vp                           2/2     Running   2          20d
pod/weave-net-fsjsv                           2/2     Running   22         15d
pod/weave-net-klgkn                           2/2     Running   48         20d
pod/weave-net-x4f8n                           2/2     Running   36         19d

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns         ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   20d
service/metrics-server   ClusterIP   10.101.226.141   <none>        443/TCP                  62s

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/kube-proxy   4         4         3       4            3           kubernetes.io/os=linux   20d
daemonset.apps/weave-net    4         4         3       4            3           <none>                   20d

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns          2/2     2            2           20d
deployment.apps/metrics-server   1/1     1            1           62s

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-74ff55c5b          2         2         2       20d
replicaset.apps/metrics-server-766c9b8df   1         1         1       63s

Now we can see those CPU/memory utilization of pods and nodes. 

=> kubectl top nodes
=> kubectl top pods


==> What is resource requests and resource limits?
Resources means CPU and Memory. How much CPU and Memory we are allocatiing to that container. and come under Contianer.  
   resource limits- Max CPU the container can take and memory. it will not go beyond this. (limits should always be equal/more than requests) 
   resource requests- Minimum gurantee of CPU and memory for our pod. 

We can apply these resource requests and resource limits to all the apllications. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
  labels:
    name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
      - name: hpscontainer
        image: k8s.gcr.io/hpa-example
        ports:
        - name: http
          containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: hpaservice
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
  selector:
    name: hpapod
---
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata: 
  name: hpadeploymentautoscaler
spec: 
  scaleTargetRef: 
    apiVersion: app/v1
    kind: Deployment
    name: hpadeployment
minReplicas: 2
maxReplicas: 5
metrics: 
  - resources: 
      name: cpu
      targetAverageUtilization: 80
    type: Resource

Lets do - kubectl apply -f hpsdemo.yml

@ To check the performance of the application before going to production testers use some softwares: here will check our application will sustain if we put this much laod. 
will find out the max load that can suatsin.

-> Apache JMeter, LoadRunner, Apache Bench, Locust, k6 etc

Here we can see two type of requirements: 
1) Functional Requirements
2) Non Functional Requirements(NFR): How much we are expected on that application. client would say this application would get 10k requests per min, then we check the 
  sustanability. Unless performace Test and regression testing we cant push that to production. 

Now lets put some temporary load with temporary pod creation. 
for temporary pod creation: "kubectl run -it --rm loadrunner --image=busybox /bin/sh"

here rm means Remove. the pod wil be removed once we come out. 











