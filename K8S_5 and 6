K8s Volume: 
-----------
It is used to store the data what we have in the application container. A container file system lives as long as the container does. so wen a container terminates and restart, 
filesystem changes are lost. Volumes in K8s are very easy to manage. its basically a directory that get mounted to a pod. after telling the container to use volumes for storing 
evergreen information, you can safely modify the pods without ever losing your data. 

K8s supports many types of volumes. Pod can use many volumes to single pod.

it supports lot of volumes
1) EmptyDir       --> its like a tmp dir. the data exist as long as the pod exists. if pod is deleted the data also get deleted. we will not lose the data if the conatiner deletes.
2) hostpath      --> Wherever our pod getting scheduled in that node/server we can attached server/host storage. 
3) nfs
4) awsElasticBlockstore
5) googlePersistantDisk
6) azureFile
7) azureDisk
8) ConfigMap
9) GitRepo(deprecated)
10)secret


There are concepts: 
1) persistentVolumes
2) persistentVolumeClaims


Example for hostPath: Now we have springapp.yml file as a stateless application. if we attache the volume it will get converted into stateful set app. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec: 
  replicas: 2
  selector: 
    matchLabels:
      app: springapp           # here if we dont mention also it will consider the RollingUpdate as default or if we want we can also
  template: 
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - image: dockerhandson/spring-boot-mango:1 
        name: springappcontainer
        ports: 
        - containerPort: 8080
        env: 
        - name: MONGO_DB_HOSTNAME
          value: mango
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSOWRD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort       #we can convert from cluster to nodeport but not from nodeport to cluster
  selector:
    app: springapp
  ports: 
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: ReplicationController
metadata: 
  name: mongorc
spec: 
  selector: 
    app: mongo
  template:
    metadata: 
      name: mongopod
      labels:
         app: mongo
    spec: 
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env: 
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb                               # This always match with springboot value
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                            # This always match with springboot value
        volumeMounts: 
        - name: mongovol                          #this name always match with volume name
          mountPath: "/data/db"    
     volumes:                                    #volume is the sibling of container. 
     - name: mongovol
       hostPath:              # mention the path type
         path: "/tmp/mongo"    #server dir where we store the data of container /data/db   
---
apiVersion: v1
kind: Service
metadata: 
  name: mongo      #This always match with MONGO_DB_HOSTNAME. why bcoz spring app will talk to mongo database. 
spec: 
  type: ClusterIP
  selector: 
    app: mongo 
  ports: 
  - port: 27017                   #Why not 80. the hardcode is 27017 in developer file. it will route the traffic to 27017 port.
    targetPort: 27017



* if any changes happens in our RC/RS, our pods will not recreted. we need to delete and create again. Now i deleted and updated new volume and apply. we can see that database in
"/tmp/mongo". 

ubuntu@ip-172-31-33-243:~$ kubectl apply -f springapp.yml
deployment.apps/springappdeployment created
service/springappsvc created
replicationcontroller/mongorc created
service/mongo created
ubuntu@ip-172-31-33-243:~$ kubectl get pods
NAME                                   READY   STATUS              RESTARTS   AGE
mongorc-zxjwf                          1/1     Running             0          6s
springappdeployment-5cfd644db9-qbd5g   0/1     ContainerCreating   0          6s
springappdeployment-5cfd644db9-qg62g   0/1     ContainerCreating   0          6s
ubuntu@ip-172-31-33-243:~$ kubectl get all
NAME                                       READY   STATUS         RESTARTS   AGE
pod/mongorc-zxjwf                          1/1     Running        0          72s
pod/springappdeployment-5cfd644db9-qbd5g   0/1     ErrImagePull   0          72s
pod/springappdeployment-5cfd644db9-qg62g   0/1     ErrImagePull   0          72s

NAME                            DESIRED   CURRENT   READY   AGE
replicationcontroller/mongorc   1         1         1       72s

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes     ClusterIP   10.96.0.1        <none>        443/TCP        5m58s
service/mongo          ClusterIP   10.111.150.154   <none>        27017/TCP      72s
service/springappsvc   NodePort    10.106.65.75     <none>        80:30694/TCP   72s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/springappdeployment   0/2     2            0           72s

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/springappdeployment-5cfd644db9   2         2         0       72s


*Above we can see the mongorc-zxjwf data base pod got created. if we want see mode details about pods. 


#Jenkins:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
spec:
  replicas: 02
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      name: jenkins-pod
      labels: 
        app: jenkins
    spec:
      containers:
      - name: jenkins-ctr
        image: jenkins/jenkins:lts
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: dev@123

---

apiVersion: v1
kind: Service
metadata:
  name: jenkins-svc
spec:
  type: NodePort
  selector: 
    app: jenkins
  ports:
  - port: 8080
    targetPort: 8080

---

apiVersion: v1
kind: ReplicationController
metadata:
  name: mongorc
spec: 
  replicas: 01
  selector:
    app: mongo
  template:
    metadata:
      name: mongo-pod
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo-ctr
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_USERNAME
          value: devdb
        - name: MONGO_INITDB_PASSWORDS
          value: devdb@123

---          
apiVersion: v1
kind: Service
metadata:
  name: mongo-svc
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017




==> Here we gave host path and if we access it first it will take one node and enter the data and it will save there and intentionally delete the pod what is it assigned that 
access and then it will schedule to another node then if we access it will not show that old data. bcoz its host path. that data will be stored in that node only. We can see data 
incosistency. if we want to maintain that data base constantly we use nfs volume. 


==> NFS- Network File System
-------------------------------
  This nfs acts like a external storage and for this we have to create one server and while creating a server we have to open 2049 port (protocol-TCP) or we can choose NFS 
directly there.  and we have to give permissions to only particular range 172.31.0.0/16 

Next open that server and do some updates.

sudo apt update -y
#install nfs software
sudo apt install nfs-kernel-server -y
#cretae a dir
sudo mkdir -p /mnt/nfs_share
# change permissions and change the ownership. 
sudo chmod 777 /mnt/nfs_share/
sudo chown -R nobody:nogroup /mnt/nfs_share/
# in one more file we have to give persmissions.
sudo vi /etc/exports

/mnt/nfs_share 172.31.0.0/16(rw,sync,no_subtree_check,no_root_squash)   # here we gave our vpc range
:wq

sudo exportfs -a
sudo systemctl restart nfs-kerner-server
sudo systemctl status nfs-kerner-server

#install nfs server in master and worker nodes
sudo apt install nfs-comman -y

#in manifest file we have to keep as it is as above but change the volumes only (springboot). in masternode 

 volumes:                                    
     - name: mongovol
       nfs:
         server: 172.31.42.  132       # this private IP of the nfs server
         path: "/mnt/nfs_share"

kubectl apply -f springapp.yml     # now our pod is mounted wit nfs server.  

kubectl describe pod mongorc-f4sjs

#Now go and check in that nfs server . we can see some files. 
 cat /mnt/nfs_share/

# now go and access that server in browser and enter data and delete that pod and it wiull be sceduled to diff node but we cann still see that old data. its called data consistancy. 

#if we want to do kubelet stop/start/status
sudo service kubelet stop
sudo service kubelet start
sudo service kubelet status


==> Persistenr Volumes (pv): Its the recommeneded way to use the volumes in K8s. here the K8s will maintian the volumes info. it will maintaine the volume info outside the pod life cycle.
its a piece of storage in our cluster. even if our pod dies we can get the volume info in k8s. 

We can use : hostpath, nfs etc here

Persistent columes are provisioned in two ways:
1) Static Volumes: As a K8s administator we need to manually create a persistent volume. 
2) Dynamic Volumes:

==> Persistent Volume Claims (pvc): i have volume created and i want to use that voulume in my pod. i ned to claim/mount that volume in my pod with help of persistent volume cliam. 
    PVC is uded to mount a PV into a POD. PVC's are a way for users to claim durable storage. 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec: 
  replicas: 2
  selector: 
    matchLabels:
      app: springapp           # here if we dont mention also it will consider the RollingUpdate as default or if we want we can also
  template: 
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - image: dockerhandson/spring-boot-mango:1 
        name: springappcontainer
        ports: 
        - containerPort: 8080
        env: 
        - name: MONGO_DB_HOSTNAME
          value: mango
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSOWRD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort       #we can convert from cluster to nodeport but not from nodeport to cluster
  selector:
    app: springapp
  ports: 
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: persistentVolumeClaim
metadata: 
  name: mongopvc
spec:
  storageClassName: manual    # here i am craeting volume manaually. we didnt use dynamic volume concept here. 
  accessModes: 
    - ReadWriteOnce
  resources:
    requests: 
      storage: 1Gi              #iGb storage
---
apiVersion: v1
kind: ReplicationController
metadata: 
  name: mongorc
spec: 
  selector: 
    app: mongo
  template:
    metadata: 
      name: mongopod
      labels:
         app: mongo
    spec: 
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env: 
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb                               # This always match with springboot value
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                            # This always match with springboot value
        volumeMounts: 
        - name: mongovol                          #this name always match with volume name
          mountPath: "/data/db"    
     volumes:                                    #volume is the sibling of container. 
     - name: mongovol
       persistentVolumeClaim:
         claimName: mongopvc                 # this claimname should match with our above claim name
        
---
apiVersion: v1
kind: Service
metadata: 
  name: mongo      #This always match with MONGO_DB_HOSTNAME. why bcoz spring app will talk to mongo database. 
spec: 
  type: ClusterIP
  selector: 
    app: mongo 
  ports: 
  - port: 27017                   #Why not 80. the hardcode is 27017 in developer file. it will route the traffic to 27017 port.
    targetPort: 27017


ubuntu@ip-172-31-34-33:~$ kubectl get pvc
NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongopvc         Pending                                      manual         33h


==> Access Modes:  (for more details https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
RWO- ReadWriteOnce - Only one node , one pod can read and write the data. 
ROX- ReadOnlyMany - Pod can read the data whatever in that storage but can write. 
RWX- ReadWriteMany - Multiple pods and multiple nodes can read and write the data. 


Above w have created a PVC but we need to PV to bound each other. PV is mandotory to create. the PVC will show in pending status until it wil attach with one PV. 

check the logs while PVC is in pending state: kubectl describe pods mongorc-f76gsh
at the end we can see the all pods unbound with volume. 

We have 2 ways to create a volumes: static and dynamic (storage class is responsible for creating a dynamic volume.)

Storage Class: its ntng but one programm/ application. whenever PVC will not find the PV, if we have our storage class configured that storage class create a PV for this PVC. 

Lets create a volume manually:

ubuntu@ip-172-31-34-33:~$ vi nfspv.yml 

apiVersion: v1
kind: PersistentVolume
metadata: 
  name: nfspv
spec:             #here we are entering volume specifications
  capacity: 
    storage: 1Gi
  accessModes: 
  - ReadWriteMany
  nfs:
    server: 172.31.43.132
     path: "/mnt/nfs_share"
  

Above we have created a PV. but it will not bound with our PVC bcoz it should match with accessmode, storage and storageclassname. in this PV and PVC concept always the PV and PVC
accessmode, storage and storageclassname will match. then only PV will get bound with PVC automatically. 
@ one PV with one PVC. 


==> Lets create host path with all matches

ubuntu@ip-172-31-34-33:~$ vi hostpath_pv.yml
apiVersion: v1
kind: PersistentVolume
metadata: 
  name: hostpathpv
spec:             #here we are entering volume specifications
  storageClassname: manual
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/mongopv"

ubuntu@ip-172-31-34-33:~$ kubectl apply -f hostpath_pv.yml
persistentvolume/hostpathpv created
ubuntu@ip-172-31-34-33:~$ kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
hostpathpv   1Gi        RWO            Retain           Bound    default/mongopvc   manual                  20s
ubuntu@ip-172-31-34-33:~$ kubectl get pvc
NAME             STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongopvc         Bound     hostpathpv   1Gi        RWO            manual         34h


Above we can see the PVC is bounded with PV. Now my pod is running. and we can access it outside also using nodeport. as we are using hostpath if we delete one pod it will 
assign to another node but that node data we cant see. 

buntu@ip-172-31-34-33:~$ kubectl get pods
NAME                                  READY   STATUS        RESTARTS   AGE
jenkins-deployment-6f7959b94f-8l7q9   1/1     Terminating   0          12d
mongorc-5tkqs                         1/1     Running       1          24h
mongorc-z5j8h                         1/1     Terminating   0          12d
nginxdeployment-6669bd848f-q5sc7      1/1     Running       1          23h


ubuntu@ip-172-31-34-33:~$ kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
hostpathpv   1Gi        RWO            Retain           Bound    default/mongopvc   manual                  20s

Above we can see the hostpathpv with RECALIM POLICY as a "Ratain". it means its default claim policy. if we want we can change also. we have 3 claim policies. 

==>RECLAIM/CLAIM POLICY:
------------------------
1)Retain: When the PVC is deleted, the PV still exist and the volume is considered "released". But its not yet available  for another cliam bcoz   the previous claims data remains 
on the volume. An administrator we can manually reclaim the volume. if we delete PVC/rc also we can see the data in PV. 
@ This is default one. 
2) Recycle: When the claim is deleted the volume remains but the performs a basic scrub (delete data from storage) (rm -rf/volume/*) on the volume and makes it available for a new
            claim. 

3) Delete: When the claim is deleted, it removes both the PV object from K8s, as well as the associated storage. the claim policy (associated at the PV and  not the PVC)
 is resposible for what happens to the data on when the claim has been deleted. it wil delete both pv and pvc. not at all store any data related to that. 

We have mention these policies while creating a PV

apiVersion: v1
kind: PersistentVolume
metadata: 
  name: hostpathpv
spec:             #here we are entering volume specifications
  persistentVolumeReclaimPolicy: Retain
  storageClassname: manual
  capacity: 
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/mongopv"


==> StorageClass: 
                  its a piece of software/application. We dont need to create a PV manually if we have Srorageclass it will create dynamically. this storage class will create 
a PV which match with PVC that yoy create. 

https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs    --> here we can  see the storage of aws-ebs. this is default one. 
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate


above we have aws-ebs and it is self managed storage class. we have to do lot of settings before set up this ebs. 

we have one more option is NFS. for NFS need to crete one server in aws and set this one as a default one. later it will create PV dynamically(automatically) after creating a PVC. 

==> We have one piece of software/application to set up a nfs storage class. plz refer below link
https://github.com/MithunTechnologiesDevOps/Kubernates-Manifests/blob/master/pv-pvc/nfsstorageclass.yml













