K8s Volume: 
-----------
It is used to store the data what we have in the application container. A container file system lives as long as the container does. so wen a container terminates and restart, 
filesystem changes are lost. Volumes in K8s are very easy to manage. its basically a directory that get mounted to a pod. after telling the container to use volumes for storing 
evergreen information, you can safely modify the pods without ever losing your data. 

K8s supports many types of volumes. Pod can use many volumes to single pod.

it supports lot of volumes
1) EmptyDir       --> its like a tmp dir. the data exist as long as the pod exists. if pod is deleted the data also get deleted. we will not lose the data if the conatiner deletes.
2) hostpath      --> Wherever our pod getting scheduled in that node/server we can attached server/host storage. 
3) nfs
4) awsElasticBlockstore
5) googlePersistantDisk
6) azureFile
7) azureDisk
8) ConfigMap
9) GitRepo(deprecated)
10)secret


There are concepts: 
1) persistentVolumes
2) persistentVolumeClaims


Example for hostPath: Now we have springapp.yml file as a stateless application. if we attache the volume it will get converted into stateful set app. 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec: 
  replicas: 2
  selector: 
    matchLabels:
      app: springapp           # here if we dont mention also it will consider the RollingUpdate as default or if we want we can also
  template: 
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - image: dockerhandson/spring-boot-mango:1 
        name: springappcontainer
        ports: 
        - containerPort: 8080
        env: 
        - name: MONGO_DB_HOSTNAME
          value: mango
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSOWRD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort       #we can convert from cluster to nodeport but not from nodeport to cluster
  selector:
    app: springapp
  ports: 
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: ReplicationController
metadata: 
  name: mongorc
spec: 
  selector: 
    app: mongo
  template:
    metadata: 
      name: mongopod
      labels:
         app: mongo
    spec: 
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env: 
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb                               # This always match with springboot value
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                            # This always match with springboot value
        volumeMounts: 
        - name: mongovol                          #this name always match with volume name
          mountPath: "/data/db"    
     volumes:                                    #volume is the sibling of container. 
     - name: mongovol
       hostPath:              # mention the path type
         path: "/tmp/mongo"    #server dir where we store the data of container /data/db   
---
apiVersion: v1
kind: Service
metadata: 
  name: mongo      #This always match with MONGO_DB_HOSTNAME. why bcoz spring app will talk to mongo database. 
spec: 
  type: ClusterIP
  selector: 
    app: mongo 
  ports: 
  - port: 27017                   #Why not 80. the hardcode is 27017 in developer file. it will route the traffic to 27017 port.
    targetPort: 27017



* if any changes happens in our RC/RS, our pods will not recreted. we need to delete and create again. Now i deleted and updated new volume and apply. we can see that database in
"/tmp/mongo". 

ubuntu@ip-172-31-33-243:~$ kubectl apply -f springapp.yml
deployment.apps/springappdeployment created
service/springappsvc created
replicationcontroller/mongorc created
service/mongo created
ubuntu@ip-172-31-33-243:~$ kubectl get pods
NAME                                   READY   STATUS              RESTARTS   AGE
mongorc-zxjwf                          1/1     Running             0          6s
springappdeployment-5cfd644db9-qbd5g   0/1     ContainerCreating   0          6s
springappdeployment-5cfd644db9-qg62g   0/1     ContainerCreating   0          6s
ubuntu@ip-172-31-33-243:~$ kubectl get all
NAME                                       READY   STATUS         RESTARTS   AGE
pod/mongorc-zxjwf                          1/1     Running        0          72s
pod/springappdeployment-5cfd644db9-qbd5g   0/1     ErrImagePull   0          72s
pod/springappdeployment-5cfd644db9-qg62g   0/1     ErrImagePull   0          72s

NAME                            DESIRED   CURRENT   READY   AGE
replicationcontroller/mongorc   1         1         1       72s

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes     ClusterIP   10.96.0.1        <none>        443/TCP        5m58s
service/mongo          ClusterIP   10.111.150.154   <none>        27017/TCP      72s
service/springappsvc   NodePort    10.106.65.75     <none>        80:30694/TCP   72s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/springappdeployment   0/2     2            0           72s

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/springappdeployment-5cfd644db9   2         2         0       72s


*Above we can see the mongorc-zxjwf data base pod got created. if we want see mode details about pods. 


#Jenkins:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
spec:
  replicas: 02
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      name: jenkins-pod
      labels: 
        app: jenkins
    spec:
      containers:
      - name: jenkins-ctr
        image: jenkins/jenkins:lts
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongo
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: dev@123

---

apiVersion: v1
kind: Service
metadata:
  name: jenkins-svc
spec:
  type: NodePort
  selector: 
    app: jenkins
  ports:
  - port: 8080
    targetPort: 8080

---

apiVersion: v1
kind: ReplicationController
metadata:
  name: mongorc
spec: 
  replicas: 01
  selector:
    app: mongo
  template:
    metadata:
      name: mongo-pod
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo-ctr
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_USERNAME
          value: devdb
        - name: MONGO_INITDB_PASSWORDS
          value: devdb@123

---          
apiVersion: v1
kind: Service
metadata:
  name: mongo-svc
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017
    targetPort: 27017




==> Here we gave host path and if we access it first it will take one node and enter the data and it will save there and intentionally delete the pod what is it assigned that 
access and then it will schedule to another node then if we access it will not show that old data. bcoz its host path. that data will be stored in that node only. We can see data 
incosistency. if we want to maintain that data base constantly we use nfs volume. 


==> NFS- Network File System
-------------------------------
  This nfs acts like a external storage and for this we have to create one server and while creating a server we have to open 2049 port (protocol-TCP) or we can choose NFS 
directly there.  and we have to give permissions to only particular range 172.31.0.0/16 

Next open that server and do some updates.

sudo apt update -y
#install nfs software
sudo apt install nfs-kernel-server -y
#cretae a dir
sudo mkdir -p /mnt/nfs_share
# change permissions and change the ownership. 
sudo chmod 777 /mnt/nfs_share/
sudo chown -R nobody:nogroup /mnt/nfs_share/
# in one more file we have to give persmissions.
sudo vi /etc/exports

/mnt/nfs_share 172.31.0.0/16(rw,sync,no_subtree_check,no_root_squash)   # here we gave our vpc range
:wq

sudo exportfs -a
sudo systemctl restart nfs-kerner-server
sudo systemctl status nfs-kerner-server

#install nfs server in master and worker nodes
sudo apt install nfs-comman -y

#in manifest file we have to keep as it is as above but change the volumes only (springboot). in masternode 

 volumes:                                    
     - name: mongovol
       nfs:
         server: 172.31.42.  132       # this private IP of the nfs server
         path: "/mnt/nfs_share"

kubectl apply -f springapp.yml     # now our pod is mounted wit nfs server.  

kubectl describe pod mongorc-f4sjs

#Now go and check in that nfs server . we can see some files. 
 cat /mnt/nfs_share/

# now go and access that server in browser and enter data and delete that pod and it wiull be sceduled to diff node but we cann still see that old data. its called data consistancy. 

#if we want to do kubelet stop/start/status
sudo service kubelet stop
sudo service kubelet start
sudo service kubelet status


==> Persistenr Volumes (pv): Its the recommeneded way to use the volumes in K8s. here the K8s will maintian the volumes info. it will maintaine the volume info outside the pod life cycle.
its a piece of storage in our cluster. even if our pod dies we can get the volume info in k8s. 

We can use : hostpath, nfs etc here

Persistent columes are provisioned in two ways:
1) Static Volumes: As a K8s administator we need to manually create a persistent volume. 
2) Dynamic Volumes:

==> Persistent Volume Claims (pvc): i have volume created and i want to use that voulume in my pod. i ned to claim/mount that volume in my pod with help of persistent volume cliam. 
    PVC is uded to mount a PV into a POD. PVC's are a way for users to claim durable storage. 


apiVersion: apps/v1
kind: Deployment
metadata:
  name: springappdeployment
spec: 
  replicas: 2
  selector: 
    matchLabels:
      app: springapp           # here if we dont mention also it will consider the RollingUpdate as default or if we want we can also
  template: 
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - image: dockerhandson/spring-boot-mango:1 
        name: springappcontainer
        ports: 
        - containerPort: 8080
        env: 
        - name: MONGO_DB_HOSTNAME
          value: mango
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSOWRD
          value: devdb@123
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort       #we can convert from cluster to nodeport but not from nodeport to cluster
  selector:
    app: springapp
  ports: 
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: persistentVolumeClaim
metadata: 
  name: mongopvc
spec:
  storageClassName: manual    # here i am craeting volume manaually. we didnt use dynamic volume concept here. 
  accessModes: 
  - ReadWriteOnce
  resources:
    requests: 
      storage: 1Gi              #iGb storage
---
apiVersion: v1
kind: ReplicationController
metadata: 
  name: mongorc
spec: 
  selector: 
    app: mongo
  template:
    metadata: 
      name: mongopod
      labels:
         app: mongo
    spec: 
      containers:
      - name: mongocontainer
        image: mongo
        ports:
        - containerPort: 27017
        env: 
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb                               # This always match with springboot value
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123                            # This always match with springboot value
        volumeMounts: 
        - name: mongovol                          #this name always match with volume name
          mountPath: "/data/db"    
     volumes:                                    #volume is the sibling of container. 
     - name: mongovol
       persistentVolumeClaim:
         claimName: mongopvc                 # this claimname should match with our above claim name
        
---
apiVersion: v1
kind: Service
metadata: 
  name: mongo      #This always match with MONGO_DB_HOSTNAME. why bcoz spring app will talk to mongo database. 
spec: 
  type: ClusterIP
  selector: 
    app: mongo 
  ports: 
  - port: 27017                   #Why not 80. the hardcode is 27017 in developer file. it will route the traffic to 27017 port.
    targetPort: 27017




==> Access Modes:  (for more details https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)
RWO- ReadWriteOnce - Only one node , one pod can read and write the data. 
ROX- ReadOnlyMany - Pod can read the data whatever in that storage but can write. 
RWX- ReadWriteMany - Multiple pods and multiple nodes can read and write the data. 


Above w have created a PVC but we need to PV to bound each other. PV is mandotory to create. the PVC will show in pending status until it wil attach with one PV. 

check the logs while PVC is in pending state: kubectl describe pods mongorc-f76gsh
at the end we can see the all pods unbound with volume. 

We have 2 ways to create a volumes: static and dynamic (storage class is responsible for creating a dynamic volume.)

Storage Class: its ntng but one programm/ application. whenever PVC will not find the PV, if we have our storage class configured that storage class create a PV for this PVC. 

Lets create a volume manually:

ubuntu@ip-172-31-34-33:~$ vi nfspv.yml 

apiVersion: v1
kind: PersistentVolume
metadata: 
  name: nfspv
spec:             #here we are entering volume specifications
  capacity: 
    storage: 1Gi
  accessModes: 
  - ReadWriteMany
  nfs:
    server: 172.31.43.132
     path: "/mnt/nfs_share"
  






apiVersion: v1
kind: PersistentVolume
metadata: 
  name: hostpathpv
spec:             #here we are entering volume specifications
  capacity: 
    storage: 1Gi
  accessModes:
    storageClassname: manual
  - ReadWriteOnce
  hostPath:
    path: "/tmp/mongo"





